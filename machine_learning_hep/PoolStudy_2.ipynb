{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pickle\n",
    "\n",
    "with open(\"default_complete.yaml\", 'r') as run_config: \n",
    "    data_config = yaml.load(run_config) \n",
    "with open(\"data/database_ml_parameters.yml\", 'r') as param_config: \n",
    "    data_param = yaml.load(param_config) \n",
    "with open(\"data/config_model_parameters.yml\", 'r') as mod_config: \n",
    "    data_model = yaml.load(mod_config) \n",
    "mcordata = \"data\"\n",
    "\n",
    "indexp = 0\n",
    "case = data_config[\"case\"]\n",
    "param_case = data_param[case]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataroot/child_1/0029/AnalysisResults.root\n",
      "dataroot/child_1/0028/AnalysisResults.root\n",
      "dataroot/child_1/0026/AnalysisResults.root\n",
      "dataroot/child_1/0030/AnalysisResults.root\n",
      "dataroot/child_1/0024/AnalysisResults.root\n",
      "dataroot/child_1/0023/AnalysisResults.root\n",
      "dataroot/child_1/0022/AnalysisResults.root\n",
      "dataroot/child_1/0025/AnalysisResults.root\n"
     ]
    }
   ],
   "source": [
    "from machine_learning_hep.listfiles import list_files_dir_lev2, list_files_lev2 \n",
    "import multiprocessing as mp\n",
    "import uproot\n",
    "\n",
    "class Processer:\n",
    "    # Class Attribute\n",
    "    species = 'processer'\n",
    "\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, datap, mcordata, indexp, maxfiles):\n",
    "        \n",
    "        self.mcordata = mcordata\n",
    "        self.index_period = indexp\n",
    "        self.maxfiles = maxfiles\n",
    "        #namefile root\n",
    "        self.n_root = datap[\"files_names\"][\"namefile_unmerged_tree\"]\n",
    "        #troot trees names\n",
    "        self.n_treereco = datap[\"files_names\"][\"treeoriginreco\"]\n",
    "        self.n_treegen = datap[\"files_names\"][\"treeorigingen\"]\n",
    "        self.n_treeevt = datap[\"files_names\"][\"treeoriginevt\"]\n",
    "        #namefiles pkl\n",
    "        self.n_reco = datap[\"files_names\"][\"namefile_reco\"]\n",
    "        self.n_evt = datap[\"files_names\"][\"namefile_evt\"]\n",
    "        self.n_gen = datap[\"files_names\"][\"namefile_gen\"]\n",
    "        #namefiles pkl skimmed\n",
    "        self.n_recosk = datap[\"files_names\"][\"namefile_reco_skim\"]\n",
    "        self.n_evtsk = datap[\"files_names\"][\"namefile_evt_skim\"]\n",
    "        self.n_gensk = datap[\"files_names\"][\"namefile_gen_skim\"]\n",
    "        #directories\n",
    "        self.d_root = datap[\"inputs\"][self.mcordata][\"unmerged_tree_dir\"][self.index_period]\n",
    "        self.d_reco = datap[\"output_folders\"][\"pkl_out\"][self.mcordata][self.index_period]\n",
    "        self.d_evt = self.d_reco\n",
    "        self.d_gen = self.d_reco\n",
    "        self.d_recosk = datap[\"output_folders\"][\"pkl_skimmed\"][self.mcordata][self.index_period]\n",
    "        self.d_evtsk = self.d_recosk\n",
    "        self.d_recosk = self.d_recosk\n",
    "        #selections\n",
    "        self.s_reco_unp = datap[\"skimming_sel\"]\n",
    "        self.s_evt_unp = datap[\"skimming_sel_evt\"]\n",
    "        self.s_gen_unp = datap[\"skimming_sel_gen\"]\n",
    "        self.s_reco_skim = datap[\"skimming2_sel\"]\n",
    "        self.s_evt_skim = datap[\"skimming2_sel_evt\"]\n",
    "        self.s_gen_skim = datap[\"skimming2_sel_gen\"]\n",
    "        self.s_reco_trackpid = datap[\"skimming2_dotrackpid\"]\n",
    "        self.s_centr = datap[\"sel_cent\"]\n",
    "        #variables name\n",
    "        self.v_all = datap[\"variables\"][\"var_all\"]\n",
    "        self.v_evt = datap[\"variables\"][\"var_evt\"][self.mcordata]\n",
    "        self.v_gen = datap[\"variables\"][\"var_all\"]\n",
    "        #list of files names\n",
    "        self.l_root = None\n",
    "        self.l_reco = None\n",
    "        self.l_gen = None\n",
    "        self.l_evt = None\n",
    "        self.l_recosk = None\n",
    "        self.l_gensk = None\n",
    "        self.l_evtsk = None\n",
    "        #parameter names\n",
    "        self.maxperchunk = 30\n",
    "        self.indexsample = None\n",
    "        \n",
    "    def set_maxperchunk(self, maxperchunk):\n",
    "        self.maxperchunk = maxperchunk\n",
    "    \n",
    "    def buildlistpkl(self):\n",
    "        self.l_root, self.l_reco = list_files_dir_lev2(self.d_root, self.d_reco, self.n_root, self.n_reco)\n",
    "        _, self.l_gen = list_files_dir_lev2(self.d_root, self.d_gen, self.n_root, self.n_gen)\n",
    "        _, self.l_evt = list_files_dir_lev2(self.d_root, self.d_evt, self.n_root, self.n_evt)\n",
    "\n",
    "    def buildlistskim(self):\n",
    "        _, self.l_recosk = list_files_dir_lev2(self.d_reco, self.d_recosk, self.n_reco, self.n_recosk)\n",
    "\n",
    "    def unpack(self, file_index):\n",
    "        print(self.l_root[file_index])\n",
    "        treereco = uproot.open(self.l_root[file_index])[self.n_treereco]\n",
    "        dfreco = treereco.pandas.df(branches=self.v_all)\n",
    "        dfreco = dfreco.query(self.s_reco_unp)\n",
    "        dfreco.to_pickle(self.l_reco[file_index])\n",
    "\n",
    "        treeevt = uproot.open(self.l_root[file_index])[self.n_treeevt]\n",
    "        dfevt = treeevt.pandas.df(branches=self.v_evt)\n",
    "        if self.s_evt_unp is not None:\n",
    "            dfevt.query(self.s_evt_unp)\n",
    "        dfevt.to_pickle(self.l_evt[file_index])\n",
    "\n",
    "        if (self.mcordata == \"mc\"):\n",
    "            treegen = uproot.open(self.l_root[file_index])[self.n_treegen]\n",
    "            dfgen = treegen.pandas.df(branches=self.v_gen)\n",
    "            dfgen = dfreco.query(self.s_gen_unp)\n",
    "            dfgen.to_pickle(self.l_gen[file_index])\n",
    "\n",
    "    def skim(self, filein, fileout):\n",
    "        df = pickle.load(open(filein, \"rb\"))\n",
    "        df = df.query(self.s_reco_skim)\n",
    "        df.to_pickle(fileout)\n",
    "    \n",
    "    def parallelizer(self, function, argument_list):\n",
    "        chunks = [argument_list[x:x+self.maxperchunk] for x in range(0, len(argument_list), self.maxperchunk)]\n",
    "        for chunk in chunks:\n",
    "            pool = mp.Pool(self.maxperchunk)\n",
    "            _ = [pool.apply(function,args=chunk[i]) for i in range(len(chunk))] \n",
    "            pool.close()\n",
    "\n",
    "    def unpacker(self):\n",
    "        self.buildlistpkl()\n",
    "        arguments = [(i,) for i in range(len(self.l_root))]\n",
    "        self.parallelizer(self.unpack, arguments)\n",
    "\n",
    "#     def skimmer(self):\n",
    "#         arguments = [(self.l_pkl[i], self.l_pklsk[i]) for i in range(len(self.l_pklsk))]\n",
    "#         self.parallelizer(self.skim, arguments)\n",
    "\n",
    "        \n",
    "myprocess = Processer(data_param[case], mcordata, indexp, 10)\n",
    "myprocess.unpacker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
